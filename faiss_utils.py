#python faiss_utils.py --check
# √≥
#python faiss_utils.py --search "hist√≤ria de Ginestar"



import os
import json
import faiss
import numpy as np
import logging

logger = logging.getLogger("FaissUtils")

# ======================================================
# üìÅ CONSTANTS DE PATHS PER DEFECTE
# ======================================================
DEFAULT_INDEX_PATH = "data/faiss_index_G_pro_large.index"
DEFAULT_METADATA_PATH = "data/corpus_original.jsonl"
DEFAULT_EMB_PATH = "data/embeddings_G_pro_large.npy"


class FaissIndex:
    def __init__(self, index: faiss.Index, documents=None):
        self.index = index
        self.documents = documents or []
        self.embeddings = None

    # ======================================================
    # üß© CREACI√ì / GUARDA / C√ÄRREGA
    # ======================================================
    @classmethod
    def load(cls,
             path: str = DEFAULT_INDEX_PATH,
             metadata_path: str = DEFAULT_METADATA_PATH,
             emb_path: str = DEFAULT_EMB_PATH,
             use_gpu: bool = False):
        """
        Carrega √≠ndex FAISS, embeddings i documents.
        Compatibilitat amb el format generat per embed_faiss.py.
        """
        if not os.path.exists(path):
            raise FileNotFoundError(f"‚ùå No s'ha trobat l'√≠ndex FAISS: {path}")

        # üß† Llegeix √≠ndex FAISS
        index = faiss.read_index(path)

        if use_gpu:
            try:
                res = faiss.StandardGpuResources()
                index = faiss.index_cpu_to_gpu(res, 0, index)
                logger.info(f"[FAISS_UTILS] √çndex carregat a GPU.")
            except Exception as e:
                logger.warning(f"[FAISS_UTILS] ‚ö†Ô∏è No s'ha pogut carregar a GPU: {e}")

        logger.info(f"[FAISS_UTILS] √çndex FAISS carregat: {index.ntotal} vectors ({index.d}D)")

        # üß© Carrega embeddings
        embeddings = None
        if os.path.exists(emb_path):
            embeddings = np.load(emb_path).astype(np.float32)
            logger.info(f"[FAISS_UTILS] Embeddings carregats: {embeddings.shape}")
        else:
            logger.warning(f"[FAISS_UTILS] ‚ö†Ô∏è No s'ha trobat {emb_path}")

        # üóÇÔ∏è Carrega metadades del corpus
        documents = []
        if os.path.exists(metadata_path):
            with open(metadata_path, "r", encoding="utf-8") as f:
                for line in f:
                    if not line.strip():
                        continue
                    try:
                        d = json.loads(line)
                        documents.append(d)
                    except json.JSONDecodeError as e:
                        logger.warning(f"[FAISS_UTILS] ‚ö†Ô∏è JSON inv√†lid: {e}")
            logger.info(f"[FAISS_UTILS] Carregats {len(documents)} documents.")
        else:
            logger.warning(f"[FAISS_UTILS] ‚ö†Ô∏è No s'ha trobat {metadata_path}")

        # üÜî Assigna IDs seq√ºencials als documents si no en tenen
        for i, d in enumerate(documents):
            if "id" not in d or d["id"] is None:
                d["id"] = i
        
        # Crea objecte FAISSIndex
        obj = cls(index=index, documents=documents)
        obj.embeddings = embeddings
        return obj

    def save(self, path: str = "faiss_G_pro.index"):
        """ Desa l‚Äô√≠ndex FAISS """
        faiss.write_index(self.index, path)
        logger.info(f"[FAISS_UTILS] √çndex FAISS guardat a {path}")

    # ======================================================
    # ‚ÑπÔ∏è INFO
    # ======================================================
    def count(self) -> int:
        """Retorna el nombre total de vectors a l‚Äô√≠ndex."""
        return self.index.ntotal

    # ======================================================
    # üßÆ COMPROVACI√ì DE COHER√àNCIA
    # ======================================================
    def check_consistency(self) -> dict:
        """
        Comprova si l'√≠ndex, els embeddings i els documents estan alineats.
        Retorna un diccionari amb les dades de coher√®ncia.
        """
        n_index = self.index.ntotal if self.index else 0
        n_emb = None if self.embeddings is None else self.embeddings.shape[0]
        n_docs = len(self.documents) if self.documents else 0

        logger.info("üìä Comprovant coher√®ncia FAISS...")
        logger.info(f"- Vectors a l'√≠ndex: {n_index}")
        logger.info(f"- Embeddings: {n_emb}")
        logger.info(f"- Documents: {n_docs}")

        if self.embeddings is None:
            logger.warning("‚ö†Ô∏è  No hi ha embeddings carregats.")
            return {"index": n_index, "embeddings": n_emb, "docs": n_docs, "ok": False}

        ok = n_index == n_emb == n_docs
        if ok:
            logger.info("‚úÖ Tot coherent: √≠ndex, embeddings i documents alineats.")
        else:
            logger.warning("‚ö†Ô∏è Inconsist√®ncia detectada!")
            logger.warning(f"   ‚Üí index.ntotal = {n_index}")
            logger.warning(f"   ‚Üí embeddings = {n_emb}")
            logger.warning(f"   ‚Üí docs = {n_docs}")

        return {"index": n_index, "embeddings": n_emb, "docs": n_docs, "ok": ok}


    # ======================================================
    # üîç CERCA DIRECTA
    # ======================================================
    def search_text(self, query_text: str, top_k: int = 10) -> list[tuple["Doc", float]]:
        """
        Cerca sem√†ntica: retorna [(Doc, score)]
        Utilitza el mateix embed_query que misCEREbot.
        """
        if self.embeddings is None or not self.documents:
            logger.warning("‚ö†Ô∏è No hi ha embeddings o documents carregats.")
            return []

        # Import intern per evitar depend√®ncies circulars
        from misCEREbot import embed_query, Doc

        qvec = embed_query(query_text).reshape(1, -1)
        norm = np.linalg.norm(qvec)
        if norm > 0:
            qvec /= norm

        scores, idxs = self.index.search(qvec.astype(np.float32), top_k)

        results = []
        for s, i in zip(scores[0], idxs[0]):
            if i < 0 or i >= len(self.documents):
                continue
            doc_data = self.documents[int(i)]

            # Si ja √©s un objecte Doc, no el tornis a crear
            if isinstance(doc_data, Doc):
                doc = doc_data
            else:
                doc_id = getattr(doc_data, "id", None) or doc_data.get("id") or int(i)
                doc = Doc(doc_data, doc_id=doc_id)

            results.append((doc, float(s)))


        return results


# ======================================================
# üöÄ EXECUCI√ì DES DE TERMINAL
# ======================================================
if __name__ == "__main__":
    import argparse
    import logging

    parser = argparse.ArgumentParser(
        description="Utilitats FAISS ‚Äî c√†rrega, comprovaci√≥ i cerques."
    )
    parser.add_argument(
        "--check",
        action="store_true",
        help="Comprova coher√®ncia entre √≠ndex, embeddings i documents.",
    )
    parser.add_argument(
        "--search",
        type=str,
        help="Fes una cerca sem√†ntica pel text indicat.",
    )
    parser.add_argument(
        "--topk",
        type=int,
        default=5,
        help="Nombre de resultats a retornar en una cerca.",
    )
    parser.add_argument(
        "--gpu",
        action="store_true",
        help="Utilitza GPU si est√† disponible per carregar l'√≠ndex.",
    )

    args = parser.parse_args()

    logging.basicConfig(
        level=logging.WARNING,
        format="%(message)s"
)


    # üîπ Carrega l'√≠ndex FAISS
    fi = FaissIndex.load(use_gpu=args.gpu)

    # üßÆ Comprovaci√≥ de coher√®ncia
    if args.check:
        info = fi.check_consistency()
        print("\nüìä RESULTAT DE COHER√àNCIA:")
        print(info)

    # üîç Cerca sem√†ntica
    if args.search:
        print(f"\nüîç Cercant: {args.search}\n")
        results = fi.search_text(args.search, top_k=args.topk)
        if not results:
            print("‚ö†Ô∏è  Cap resultat trobat.")
        else:
            for i, (doc, score) in enumerate(results, start=1):
                title = getattr(doc, "title", "Sense t√≠tol")
                doc_id = getattr(doc, "id", None)
                print(f"[{title}](https://t.me/misCEREbot/{doc_id}) (Pes: {score:.2f})")
